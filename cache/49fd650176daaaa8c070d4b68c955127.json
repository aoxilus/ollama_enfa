{
    "context":  "=== C:\\Users\\aoxil\\OneDrive - El Paso Community College\\Documents\\ollama_desktop_cursorAI\\python\\ollama_cache.py ===\n#!/usr/bin/env python3\r\n\"\"\"\r\nOllama Cache System - Improve performance with response caching\r\n\"\"\"\r\n\r\nimport hashlib\r\nimport json\r\nimport os\r\nimport time\r\nfrom datetime import datetime, timedelta\r\nfrom typing import Optional, Dict, Any\r\n\r\nclass OllamaCache:\r\n    def __init__(self, cache_dir: str = \"cache\", max_age_hours: int = 24):\r\n        self.cache_dir = cache_dir\r\n        self.max_age_seconds = max_age_hours * 3600\r\n        self._ensure_cache_dir()\r\n    \r\n    def _ensure_cache_dir(self):\r\n        \"\"\"Ensure cache directory exists\"\"\"\r\n        if not os.path.exists(self.cache_dir):\r\n            os.makedirs(self.cache_dir)\r\n    \r\n    def _generate_cache_key(self, question: str, context: str = \"\", model: str = \"\") -\u003e str:\r\n        \"\"\"Generate unique cache key from question, context and model\"\"\"\r\n        content = f\"{question}|{context}|{model}\"\r\n        return hashlib.md5(content.encode(\u0027utf-8\u0027)).hexdigest()\r\n    \r\n    def _get_cache_file_path(self, cache_key: str) -\u003e str:\r\n        \"\"\"Get full path for cache file\"\"\"\r\n        return os.path.join(self.cache_dir, f\"{cache_key}.json\")\r\n    \r\n    def get(self, question: str, context: str = \"\", model: str = \"\") -\u003e Optional[str]:\r\n        \"\"\"Get cached response if available and not expired\"\"\"\r\n        cache_key = self._generate_cache_key(question, context, model)\r\n        cache_file = self._get_cache_file_path(cache_key)\r\n        \r\n        if not os.path.exists(cache_file):\r\n            return None\r\n        \r\n        try:\r\n            with open(cache_file, \u0027r\u0027, encoding=\u0027utf-8\u0027) as f:\r\n                cache_data = json.load(f)\r\n            \r\n            # Check if cache is expired\r\n            cache_time = datetime.fromisoformat(cache_data[\u0027timestamp\u0027])\r\n            if datetime.now() - cache_time \u003e timedelta(seconds=self.max_age_seconds):\r\n                os.remove(cache_file)  # Remove expired cache\r\n                return None\r\n            \r\n            return cache_data[\u0027response\u0027]\r\n            \r\n        except (json.JSON...\n\n=== C:\\Users\\aoxil\\OneDrive - El Paso Community College\\Documents\\ollama_desktop_cursorAI\\python\\ollama_errors.py ===\n#!/usr/bin/env python3\r\n\"\"\"\r\nOllama Error Handling System - Improve robustness with comprehensive error handling\r\n\"\"\"\r\n\r\nimport asyncio\r\nimport aiohttp\r\nimport json\r\nimport os\r\nimport sys\r\nimport traceback\r\nfrom datetime import datetime\r\nfrom typing import Optional, Dict, Any, Union\r\nfrom enum import Enum\r\n\r\nclass ErrorType(Enum):\r\n    \"\"\"Error types for categorization\"\"\"\r\n    CONNECTION = \"connection\"\r\n    TIMEOUT = \"timeout\"\r\n    AUTHENTICATION = \"authentication\"\r\n    VALIDATION = \"validation\"\r\n    RATE_LIMIT = \"rate_limit\"\r\n    MODEL_NOT_FOUND = \"model_not_found\"\r\n    INVALID_RESPONSE = \"invalid_response\"\r\n    FILE_SYSTEM = \"file_system\"\r\n    NETWORK = \"network\"\r\n    UNKNOWN = \"unknown\"\r\n\r\nclass OllamaError(Exception):\r\n    \"\"\"Base exception for Ollama-related errors\"\"\"\r\n    \r\n    def __init__(self, message: str, error_type: ErrorType, details: Optional[Dict[str, Any]] = None):\r\n        super().__init__(message)\r\n        self.error_type = error_type\r\n        self.details = details or {}\r\n        self.timestamp = datetime.now()\r\n    \r\n    def __str__(self):\r\n        return f\"[{self.error_type.value.upper()}] {super().__str__()}\"\r\n    \r\n    def to_dict(self) -\u003e Dict[str, Any]:\r\n        \"\"\"Convert error to dictionary for logging\"\"\"\r\n        return {\r\n            \u0027error_type\u0027: self.error_type.value,\r\n            \u0027message\u0027: str(self),\r\n            \u0027details\u0027: self.details,\r\n            \u0027timestamp\u0027: self.timestamp.isoformat()\r\n        }\r\n\r\nclass ConnectionError(OllamaError):\r\n    \"\"\"Connection-related errors\"\"\"\r\n    def __init__(self, message: str, details: Optional[Dict[str, Any]] = None):\r\n        super().__init__(message, ErrorType.CONNECTION, details)\r\n\r\nclass TimeoutError(OllamaError):\r\n    \"\"\"Timeout-related errors\"\"\"\r\n    def __init__(self, message: str, details: Optional[Dict[str, Any]] = None):\r\n        super().__init__(message, ErrorType.TIMEOUT, details)\r\n\r\nclass ValidationError(OllamaError):\r\n    \"\"\"Validation-related errors\"\"\"\r\n    def __init__(self, messag...\n\n=== C:\\Users\\aoxil\\OneDrive - El Paso Community College\\Documents\\ollama_desktop_cursorAI\\python\\ollama_simple_async.py ===\n#!/usr/bin/env python3\r\n\"\"\"\r\nOllama Simple Async - Enhanced version with cache, error handling, and validation\r\nUsage: python ollama_simple_async.py \"your question\" [path_to_analyze]\r\n\r\nConfiguration:\r\n- SYNC_MODE: Set to True to enable synchronous mode (blocking)\r\n- Set to False for asynchronous mode (non-blocking, recommended)\r\n\"\"\"\r\nimport asyncio\r\nimport aiohttp\r\nimport json\r\nimport os\r\nimport sys\r\nimport glob\r\nfrom datetime import datetime\r\nfrom concurrent.futures import ThreadPoolExecutor\r\n\r\n# Configuration - Set to True for sync mode, False for async mode (recommended)\r\nSYNC_MODE = False\r\n\r\n# Import our new modules\r\nfrom ollama_cache import get_cached_response, cache_response, get_cache_stats\r\nfrom ollama_errors import (\r\n    handle_error, validate_question, validate_model, validate_file_path,\r\n    retry_operation, check_ollama_health, get_system_info, get_error_stats\r\n)\r\n\r\nasync def check_ollama_connection():\r\n    \"\"\"Check if Ollama is running and get the best available model\"\"\"\r\n    try:\r\n        async with aiohttp.ClientSession() as session:\r\n            async with session.get(\"http://localhost:11434/api/tags\", timeout=aiohttp.ClientTimeout(total=5)) as response:\r\n                if response.status == 200:\r\n                    data = await response.json()\r\n                    models = data.get(\u0027models\u0027, [])\r\n                    \r\n                    if not models:\r\n                        return False, \"No hay modelos disponibles en Ollama\"\r\n                    \r\n                    # Find the largest model (most parameters)\r\n                    best_model = None\r\n                    largest_size = 0\r\n                    \r\n                    for model in models:\r\n                        size_bytes = model.get(\u0027size\u0027, 0)\r\n                        size_mb = size_bytes / (1024 * 1024)\r\n                        \r\n                        if size_mb \u003e largest_size:\r\n                            largest_size = size_mb\r\n                            best_model = model[...\n\n=== C:\\Users\\aoxil\\OneDrive - El Paso Community College\\Documents\\ollama_desktop_cursorAI\\tests\\test_ollama.py ===\n#!/usr/bin/env python3\r\n\"\"\"\r\nAutomated Tests for Ollama Desktop Cursor AI\r\n\"\"\"\r\n\r\nimport unittest\r\nimport asyncio\r\nimport aiohttp\r\nimport json\r\nimport os\r\nimport sys\r\nfrom unittest.mock import Mock, patch, AsyncMock\r\nfrom datetime import datetime\r\n\r\n# Add parent directory to path for imports\r\nsys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\r\n\r\nfrom python.ollama_cache import OllamaCache, get_cached_response, cache_response, clear_cache, get_cache_stats\r\nfrom python.ollama_simple_async import check_ollama_connection, get_project_files, query_ollama\r\n\r\nclass TestOllamaCache(unittest.TestCase):\r\n    \"\"\"Test cache functionality\"\"\"\r\n    \r\n    def setUp(self):\r\n        \"\"\"Set up test environment\"\"\"\r\n        self.test_cache_dir = \"test_cache\"\r\n        self.cache = OllamaCache(self.test_cache_dir, max_age_hours=1)\r\n    \r\n    def tearDown(self):\r\n        \"\"\"Clean up test environment\"\"\"\r\n        import shutil\r\n        if os.path.exists(self.test_cache_dir):\r\n            shutil.rmtree(self.test_cache_dir)\r\n    \r\n    def test_cache_key_generation(self):\r\n        \"\"\"Test cache key generation\"\"\"\r\n        key1 = self.cache._generate_cache_key(\"test question\", \"test context\", \"test model\")\r\n        key2 = self.cache._generate_cache_key(\"test question\", \"test context\", \"test model\")\r\n        key3 = self.cache._generate_cache_key(\"different question\", \"test context\", \"test model\")\r\n        \r\n        self.assertEqual(key1, key2)  # Same inputs should generate same key\r\n        self.assertNotEqual(key1, key3)  # Different inputs should generate different keys\r\n    \r\n    def test_cache_set_and_get(self):\r\n        \"\"\"Test setting and getting cache entries\"\"\"\r\n        question = \"What is 2+2?\"\r\n        response = \"The answer is 4\"\r\n        context = \"Math context\"\r\n        model = \"test-model\"\r\n        \r\n        # Set cache\r\n        self.cache.set(question, response, context, model)\r\n        \r\n        # Get cache\r\n        cached_response = self.cache.get(questi...\n\n=== C:\\Users\\aoxil\\OneDrive - El Paso Community College\\Documents\\ollama_desktop_cursorAI\\complaint_form.html ===\n\u003c!DOCTYPE html\u003e\r\n\u003chtml\u003e\r\n\u003chead\u003e\r\n    \u003ctitle\u003eComplaint Form\u003c/title\u003e\r\n\u003c/head\u003e\r\n\u003cbody\u003e\r\n    \u003ch1\u003eComplaint Form\u003c/h1\u003e\r\n    \u003cform id=\"complaintForm\"\u003e\r\n        \u003clabel for=\"name\"\u003eName:\u003c/label\u003e\r\n        \u003cinput type=\"text\" id=\"name\" name=\"name\" required\u003e\u003cbr\u003e\u003cbr\u003e\r\n        \r\n        \u003clabel for=\"email\"\u003eEmail:\u003c/label\u003e\r\n        \u003cinput type=\"email\" id=\"email\" name=\"email\" required\u003e\u003cbr\u003e\u003cbr\u003e\r\n        \r\n        \u003clabel for=\"complaint\"\u003eComplaint:\u003c/label\u003e\r\n        \u003ctextarea id=\"complaint\" name=\"complaint\" rows=\"4\" cols=\"50\" required\u003e\u003c/textarea\u003e\u003cbr\u003e\u003cbr\u003e\r\n        \r\n        \u003cbutton type=\"submit\"\u003eSubmit Complaint\u003c/button\u003e\r\n    \u003c/form\u003e\r\n\r\n    \u003cdiv id=\"result\"\u003e\u003c/div\u003e\r\n\r\n    \u003cscript\u003e\r\n        document.getElementById(\u0027complaintForm\u0027).addEventListener(\u0027submit\u0027, function(e) {\r\n            e.preventDefault();\r\n            \r\n            const name = document.getElementById(\u0027name\u0027).value;\r\n            const email = document.getElementById(\u0027email\u0027).value;\r\n            const complaint = document.getElementById(\u0027complaint\u0027).value;\r\n            \r\n            const result = document.getElementById(\u0027result\u0027);\r\n            result.innerHTML = `\r\n                \u003ch3\u003eComplaint Submitted:\u003c/h3\u003e\r\n                \u003cp\u003e\u003cstrong\u003eName:\u003c/strong\u003e ${name}\u003c/p\u003e\r\n                \u003cp\u003e\u003cstrong\u003eEmail:\u003c/strong\u003e ${email}\u003c/p\u003e\r\n                \u003cp\u003e\u003cstrong\u003eComplaint:\u003c/strong\u003e ${complaint}\u003c/p\u003e\r\n            `;\r\n            \r\n            // Clear form\r\n            document.getElementById(\u0027complaintForm\u0027).reset();\r\n        });\r\n    \u003c/script\u003e\r\n\u003c/body\u003e\r\n\u003c/html\u003e...\n\n=== C:\\Users\\aoxil\\OneDrive - El Paso Community College\\Documents\\ollama_desktop_cursorAI\\complaint_form_improved.html ===\n\u003c!DOCTYPE html\u003e\r\n\u003chtml lang=\"en\"\u003e\r\n\u003chead\u003e\r\n    \u003cmeta charset=\"UTF-8\"\u003e\r\n    \u003cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"\u003e\r\n    \u003ctitle\u003eComplaint Form - Improved\u003c/title\u003e\r\n    \u003cstyle\u003e\r\n        body { font-family: Arial, sans-serif; max-width: 600px; margin: 0 auto; padding: 20px; }\r\n        .form-group { margin-bottom: 15px; }\r\n        label { display: block; margin-bottom: 5px; font-weight: bold; }\r\n        input, textarea { width: 100%; padding: 8px; border: 1px solid #ddd; border-radius: 4px; }\r\n        .error { color: red; font-size: 14px; margin-top: 5px; }\r\n        .success { color: green; font-size: 14px; margin-top: 5px; }\r\n        button { background: #007bff; color: white; padding: 10px 20px; border: none; border-radius: 4px; cursor: pointer; }\r\n        button:hover { background: #0056b3; }\r\n        button:disabled { background: #ccc; cursor: not-allowed; }\r\n    \u003c/style\u003e\r\n\u003c/head\u003e\r\n\u003cbody\u003e\r\n    \u003ch1\u003eComplaint Form - Improved\u003c/h1\u003e\r\n    \r\n    \u003cform id=\"complaintForm\" novalidate\u003e\r\n        \u003cdiv class=\"form-group\"\u003e\r\n            \u003clabel for=\"name\"\u003eName: *\u003c/label\u003e\r\n            \u003cinput type=\"text\" id=\"name\" name=\"name\" required \r\n                   aria-describedby=\"name-error\" \r\n                   pattern=\"[A-Za-z\\s]{2,50}\"\r\n                   title=\"Name must be 2-50 characters, letters and spaces only\"\u003e\r\n            \u003cdiv id=\"name-error\" class=\"error\" role=\"alert\"\u003e\u003c/div\u003e\r\n        \u003c/div\u003e\r\n        \r\n        \u003cdiv class=\"form-group\"\u003e\r\n            \u003clabel for=\"email\"\u003eEmail: *\u003c/label\u003e\r\n            \u003cinput type=\"email\" id=\"email\" name=\"email\" required \r\n                   aria-describedby=\"email-error\"\u003e\r\n            \u003cdiv id=\"email-error\" class=\"error\" role=\"alert\"\u003e\u003c/div\u003e\r\n        \u003c/div\u003e\r\n        \r\n        \u003cdiv class=\"form-group\"\u003e\r\n            \u003clabel for=\"complaint\"\u003eComplaint: *\u003c/label\u003e\r\n            \u003ctextarea id=\"complaint\" name=\"complaint\" rows=\"4\" required \r\n                      aria-describedby=\"complaint-error\"\r\n                      minlength=\"10\" ma...\n\n=== C:\\Users\\aoxil\\OneDrive - El Paso Community College\\Documents\\ollama_desktop_cursorAI\\cache\\1e3dd4221ea52504aee90e4c0291ab4d.json ===\n{\r\n  \"question\": \"What is 3+3\",\r\n  \"response\": \"=== .\\\\python\\\\ollama_cache.py ===\\n#!/usr/bin/env python3\\n\\\"\\\"\\\"\\nOllama Cache System - Improve performance with response caching\\n\\\"\\\"\\\"\\n\\nimport hashlib\\nimport json\\nimport os\\nimport time\\nfrom datetime import datetime, timedelta\\nfrom typing import Optional, Dict, Any\\n\\nclass OllamaCache:\\n    def __init__(self, cache_dir: str = \\\"cache\\\", max_age_hours: int = 24):\\n        self.cache_dir = cache_dir\\n        self.max_age_seconds = max_age_hours * 3600\\n        self._ensure_cache_dir()\\n    \\n    def _ensure_cache_dir(self):\\n        \\\"\\\"\\\"Ensure cache directory exists\\\"\\\"\\\"\\n        if not os.path.exists(self.cache_dir):\\n            os.makedirs(self.cache_dir)\\n    \\n    def _generate_cache_key(self, question: str, context: str = \\\"\\\", model: str = \\\"\\\") -\u003e str:\\n        \\\"\\\"\\\"Generate unique cache key from question, context and model\\\"\\\"\\\"\\n        content = f\\\"{question}|{context}|{model}\\\"\\n        return hashlib.md5(content.encode(\u0027utf-8\u0027)).hexdigest()\\n    \\n    def _get_cache_file_path(self, cache_key: str) -\u003e str:\\n        \\\"\\\"\\\"Get full path for cache file\\\"\\\"\\\"\\n        return os.path.join(self.cache_dir, f\\\"{cache_key}.json\\\")\\n    \\n    def get(self, question: str, context: str = \\\"\\\", model: str = \\\"\\\") -\u003e Optional[str]:\\n        \\\"\\\"\\\"Get cached response if available and not expired\\\"\\\"\\\"\\n        cache_key = self._generate_cache_key(question, context, model)\\n        cache_file = self._get_cache_file_path(cache_key)\\n        \\n        if not os.path.exists(cache_file):\\n            return None\\n        \\n        try:\\n            with open(cache_file, \u0027r\u0027, encoding=\u0027utf-8\u0027) as f:\\n                cache_data = json.load(f)\\n            \\n            # Check if cache is expired\\n            cache_time = datetime.fromisoformat(cache_data[\u0027timestamp\u0027])\\n            if datetime.now() - cache_time \u003e timedelta(seconds=self.max_age_seconds):\\n                os.remove(cache_file)  # Remove expired cach...\n\n=== C:\\Users\\aoxil\\OneDrive - El Paso Community College\\Documents\\ollama_desktop_cursorAI\\cache\\6f43844a20013c52c2b280354bb6045d.json ===\n{\r\n  \"question\": \"What is 2+2\",\r\n  \"response\": \"=== .\\\\python\\\\ollama_cache.py ===\\n#!/usr/bin/env python3\\n\\\"\\\"\\\"\\nOllama Cache System - Improve performance with response caching\\n\\\"\\\"\\\"\\n\\nimport hashlib\\nimport json\\nimport os\\nimport time\\nfrom datetime import datetime, timedelta\\nfrom typing import Optional, Dict, Any\\n\\nclass OllamaCache:\\n    def __init__(self, cache_dir: str = \\\"cache\\\", max_age_hours: int = 24):\\n        self.cache_dir = cache_dir\\n        self.max_age_seconds = max_age_hours * 3600\\n        self._ensure_cache_dir()\\n    \\n    def _ensure_cache_dir(self):\\n        \\\"\\\"\\\"Ensure cache directory exists\\\"\\\"\\\"\\n        if not os.path.exists(self.cache_dir):\\n            os.makedirs(self.cache_dir)\\n    \\n    def _generate_cache_key(self, question: str, context: str = \\\"\\\", model: str = \\\"\\\") -\u003e str:\\n        \\\"\\\"\\\"Generate unique cache key from question, context and model\\\"\\\"\\\"\\n        content = f\\\"{question}|{context}|{model}\\\"\\n        return hashlib.md5(content.encode(\u0027utf-8\u0027)).hexdigest()\\n    \\n    def _get_cache_file_path(self, cache_key: str) -\u003e str:\\n        \\\"\\\"\\\"Get full path for cache file\\\"\\\"\\\"\\n        return os.path.join(self.cache_dir, f\\\"{cache_key}.json\\\")\\n    \\n    def get(self, question: str, context: str = \\\"\\\", model: str = \\\"\\\") -\u003e Optional[str]:\\n        \\\"\\\"\\\"Get cached response if available and not expired\\\"\\\"\\\"\\n        cache_key = self._generate_cache_key(question, context, model)\\n        cache_file = self._get_cache_file_path(cache_key)\\n        \\n        if not os.path.exists(cache_file):\\n            return None\\n        \\n        try:\\n            with open(cache_file, \u0027r\u0027, encoding=\u0027utf-8\u0027) as f:\\n                cache_data = json.load(f)\\n            \\n            # Check if cache is expired\\n            cache_time = datetime.fromisoformat(cache_data[\u0027timestamp\u0027])\\n            if datetime.now() - cache_time \u003e timedelta(seconds=self.max_age_seconds):\\n                os.remove(cache_file)  # Remove expired cach...\n\n=== C:\\Users\\aoxil\\OneDrive - El Paso Community College\\Documents\\ollama_desktop_cursorAI\\cache\\8532e7b952343ff7a4fa34d80d814a3d.json ===\n{\r\n  \"question\": \"What is 3+3\",\r\n  \"response\": \"The answer to the question \\\"What is 3+3?\\\" is 6.\",\r\n  \"context\": \"=== .\\\\python\\\\ollama_cache.py ===\\n#!/usr/bin/env python3\\n\\\"\\\"\\\"\\nOllama Cache System - Improve performance with response caching\\n\\\"\\\"\\\"\\n\\nimport hashlib\\nimport json\\nimport os\\nimport time\\nfrom datetime import datetime, timedelta\\nfrom typing import Optional, Dict, Any\\n\\nclass OllamaCache:\\n    def __init__(self, cache_dir: str = \\\"cache\\\", max_age_hours: int = 24):\\n        self.cache_dir = cache_dir\\n        self.max_age_seconds = max_age_hours * 3600\\n        self._ensure_cache_dir()\\n    \\n    def _ensure_cache_dir(self):\\n        \\\"\\\"\\\"Ensure cache directory exists\\\"\\\"\\\"\\n        if not os.path.exists(self.cache_dir):\\n            os.makedirs(self.cache_dir)\\n    \\n    def _generate_cache_key(self, question: str, context: str = \\\"\\\", model: str = \\\"\\\") -\u003e str:\\n        \\\"\\\"\\\"Generate unique cache key from question, context and model\\\"\\\"\\\"\\n        content = f\\\"{question}|{context}|{model}\\\"\\n        return hashlib.md5(content.encode(\u0027utf-8\u0027)).hexdigest()\\n    \\n    def _get_cache_file_path(self, cache_key: str) -\u003e str:\\n        \\\"\\\"\\\"Get full path for cache file\\\"\\\"\\\"\\n        return os.path.join(self.cache_dir, f\\\"{cache_key}.json\\\")\\n    \\n    def get(self, question: str, context: str = \\\"\\\", model: str = \\\"\\\") -\u003e Optional[str]:\\n        \\\"\\\"\\\"Get cached response if available and not expired\\\"\\\"\\\"\\n        cache_key = self._generate_cache_key(question, context, model)\\n        cache_file = self._get_cache_file_path(cache_key)\\n        \\n        if not os.path.exists(cache_file):\\n            return None\\n        \\n        try:\\n            with open(cache_file, \u0027r\u0027, encoding=\u0027utf-8\u0027) as f:\\n                cache_data = json.load(f)\\n            \\n            # Check if cache is expired\\n            cache_time = datetime.fromisoformat(cache_data[\u0027timestamp\u0027])\\n            if datetime.now() - cache_time \u003e timedelta(seconds=self.max_age_seco...\n\n=== C:\\Users\\aoxil\\OneDrive - El Paso Community College\\Documents\\ollama_desktop_cursorAI\\cache\\baaa092f7b42d9a891551aed1ebedd44.json ===\n{\r\n  \"question\": \"What is 2+2\",\r\n  \"response\": \"The answer to the question \\\"What is 2+2?\\\" is 4.\",\r\n  \"context\": \"=== .\\\\python\\\\ollama_cache.py ===\\n#!/usr/bin/env python3\\n\\\"\\\"\\\"\\nOllama Cache System - Improve performance with response caching\\n\\\"\\\"\\\"\\n\\nimport hashlib\\nimport json\\nimport os\\nimport time\\nfrom datetime import datetime, timedelta\\nfrom typing import Optional, Dict, Any\\n\\nclass OllamaCache:\\n    def __init__(self, cache_dir: str = \\\"cache\\\", max_age_hours: int = 24):\\n        self.cache_dir = cache_dir\\n        self.max_age_seconds = max_age_hours * 3600\\n        self._ensure_cache_dir()\\n    \\n    def _ensure_cache_dir(self):\\n        \\\"\\\"\\\"Ensure cache directory exists\\\"\\\"\\\"\\n        if not os.path.exists(self.cache_dir):\\n            os.makedirs(self.cache_dir)\\n    \\n    def _generate_cache_key(self, question: str, context: str = \\\"\\\", model: str = \\\"\\\") -\u003e str:\\n        \\\"\\\"\\\"Generate unique cache key from question, context and model\\\"\\\"\\\"\\n        content = f\\\"{question}|{context}|{model}\\\"\\n        return hashlib.md5(content.encode(\u0027utf-8\u0027)).hexdigest()\\n    \\n    def _get_cache_file_path(self, cache_key: str) -\u003e str:\\n        \\\"\\\"\\\"Get full path for cache file\\\"\\\"\\\"\\n        return os.path.join(self.cache_dir, f\\\"{cache_key}.json\\\")\\n    \\n    def get(self, question: str, context: str = \\\"\\\", model: str = \\\"\\\") -\u003e Optional[str]:\\n        \\\"\\\"\\\"Get cached response if available and not expired\\\"\\\"\\\"\\n        cache_key = self._generate_cache_key(question, context, model)\\n        cache_file = self._get_cache_file_path(cache_key)\\n        \\n        if not os.path.exists(cache_file):\\n            return None\\n        \\n        try:\\n            with open(cache_file, \u0027r\u0027, encoding=\u0027utf-8\u0027) as f:\\n                cache_data = json.load(f)\\n            \\n            # Check if cache is expired\\n            cache_time = datetime.fromisoformat(cache_data[\u0027timestamp\u0027])\\n            if datetime.now() - cache_time \u003e timedelta(seconds=self.max_age_seco...\n\n=== C:\\Users\\aoxil\\OneDrive - El Paso Community College\\Documents\\ollama_desktop_cursorAI\\cache\\f9f6ea642336b25761700a6ffbc14588.json ===\n{\r\n  \"question\": \"integration test question\",\r\n  \"response\": \"integration test response\",\r\n  \"context\": \"\",\r\n  \"model\": \"\",\r\n  \"timestamp\": \"2025-07-28T17:22:07.052795\",\r\n  \"cache_key\": \"f9f6ea642336b25761700a6ffbc14588\"\r\n}...\n\n=== C:\\Users\\aoxil\\OneDrive - El Paso Community College\\Documents\\ollama_desktop_cursorAI\\logs\\ollama_responses.txt ===\n\n=== 2025-07-28 17:22:57 ===\nQ: What is 2+2\nA: The answer to the question \"What is 2+2?\" is 4.\n\n=== 2025-07-28 17:23:19 ===\nQ: What is 3+3\nA: The answer to the question \"What is 3+3?\" is 6.\n...\n\n=== C:\\Users\\aoxil\\OneDrive - El Paso Community College\\Documents\\ollama_desktop_cursorAI\\python\\requirements.txt ===\nrequests\u003e=2.25.1\r\naiohttp\u003e=3.8.0\r\npytest\u003e=6.0.0\r\npytest-asyncio\u003e=0.18.0\r\npytest-mock\u003e=3.6.0 ...\n",
    "created":  "2025-07-28 17:23:43",
    "question":  "What is 4+4",
    "response":  "The answer to the question \"What is 4+4?\" is 8."
}
