# Ollama Desktop Cursor AI Integration

> Developed by aoxilus to connect Ollama AI to Cursor AI via terminal. Can be used with other SDKs and serves as an example for other AI integrations.

A comprehensive solution for integrating local Ollama instances with Cursor AI and development tools. Provides seamless connection between any SDK and terminal for local Ollama usage.

## ğŸš€ Ollama Local Setup

### Installation
1. **Ollama installed**: âœ… `smollm2:135m` (270MB)
2. **Service running**: âœ… Port 11434
3. **GPU active**: âœ… 100% GPU usage

### Available Model
- **smollm2:135m**: Local model with 135M parameters
- **Size**: 270MB
- **Endpoint**: http://localhost:11434

## ğŸ¤– Ollama Context Tool

### Installation
```bash
# Python version
pip install -r python/requirements.txt

# PowerShell version (Included with Windows)
# No additional installation required
```

### Usage

#### ğŸ Python Version
```bash
# Using Python directly
python python/ollama_simple.py "How does authentication work?"

# Using batch script (Windows)
ollama_simple.bat "How does authentication work?"
```

#### âš¡ PowerShell Version
```bash
# Using PowerShell directly
powershell -ExecutionPolicy Bypass -File "powershell/ollama_simple.ps1" "What files are in this project?"

# Using batch script (Windows)
ollama_simple_ps.bat "How to optimize this function?"
```

### Features
- âœ… **Context analysis**: Reads project files
- âœ… **Response buffer**: Saves last 10 queries
- âœ… **Output file**: `ollama_responses.txt`
- âœ… **Multiple formats**: Python, JS, HTML, CSS, PHP, etc.
- âœ… **Terminal colors**: Colored output

### Usage Examples
```bash
# Code-related questions
ollama_simple.bat "What does the main() function do?"
ollama_simple_ps.bat "How does it connect to the database?"
ollama_simple.bat "What is the project structure?"

# Problem analysis
ollama_simple_ps.bat "Why does the login fail?"
ollama_simple.bat "How to optimize this function?"

# General questions
ollama_simple_ps.bat "What is the capital of France?"
```

## ğŸ“ Project Structure

```
ollama_desktop_cursorAI/
â”œâ”€â”€ ğŸ python/
â”‚   â”œâ”€â”€ ollama_simple.py      # Simplified version
â”‚   â”œâ”€â”€ ollama_context.py     # Complete version
â”‚   â””â”€â”€ requirements.txt      # Dependencies
â”œâ”€â”€ âš¡ powershell/
â”‚   â”œâ”€â”€ ollama_simple.ps1     # Simplified version
â”‚   â””â”€â”€ ollama_context.ps1    # Complete version
â”œâ”€â”€ ğŸš€ Batch Scripts
â”‚   â”œâ”€â”€ ollama_simple.bat     # Python simple
â”‚   â”œâ”€â”€ ollama_context.bat    # Python complete
â”‚   â”œâ”€â”€ ollama_simple_ps.bat  # PowerShell simple
â”‚   â””â”€â”€ ollama_context_ps.bat # PowerShell complete
â”œâ”€â”€ ğŸ“„ Configuration
â”‚   â”œâ”€â”€ .cursorrules          # Programming rules
â”‚   â”œâ”€â”€ .cursor/settings.json # Cursor AI configuration
â”‚   â””â”€â”€ README.md             # This file
â””â”€â”€ ğŸ“ Output
    â””â”€â”€ ollama_responses.txt  # Response buffer
```

## ğŸ”§ Cursor AI Configuration

**Note**: Cursor does not have native Ollama support. However, we provide local tools:

1. **For quick queries**: `ollama_simple.bat "your question"`
2. **For development**: Use Cursor's official models
3. **For deep analysis**: Use Ollama directly with context

## ğŸ“Š System Status

```bash
# Verify Ollama
ollama list
ollama ps

# Test connection
curl http://localhost:11434/api/tags
```

## ğŸ¯ Recommended Usage

1. **Cursor AI**: For general development and powerful models
2. **Ollama Context**: For project-specific queries
3. **Ollama Direct**: For experimentation and prototyping

## ğŸš€ Quick Commands

```bash
# Start Ollama (if not running)
ollama serve

# Quick query with Python
ollama_simple.bat "What does this code do?"

# Quick query with PowerShell
ollama_simple_ps.bat "How to optimize this function?"

# View saved responses
type ollama_responses.txt
```

## ğŸ¯ Use Cases

### Code Analysis
```bash
ollama_simple.bat "What does the main() function do?"
ollama_simple.bat "How does it connect to the database?"
ollama_simple.bat "What is the project structure?"
```

### Debugging
```bash
ollama_simple.bat "Why does the login fail?"
ollama_simple.bat "How to optimize this function?"
ollama_simple.bat "Are there errors in this code?"
```

### Documentation
```bash
ollama_simple.bat "How to document this function?"
ollama_simple.bat "What comments to add to this code?"
```

## ğŸ› Troubleshooting

### Ollama not responding
```bash
# Check if it's running
ollama ps

# Restart Ollama
ollama serve
```

### Python error
```bash
# Install dependencies
pip install -r python/requirements.txt
```

### PowerShell error
```bash
# Allow script execution
Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser
```

## ğŸ¤– Technical Architecture (For AI Understanding)

### System Components

#### 1. **Ollama Local Server**
- **Function**: Local AI model server
- **Endpoint**: `http://localhost:11434`
- **Model**: `smollm2:135m` (135M parameters, 270MB)
- **Protocol**: REST API with `/api/generate` and `/api/tags` endpoints

#### 2. **Integration Scripts**

**Python (`ollama_simple.py`)**
```python
# Execution flow:
1. check_ollama_connection() â†’ Verify service + detect best model
2. get_project_files() â†’ Read project files (max 10, <50KB)
3. query_ollama() â†’ Send prompt with context to API
4. save_response() â†’ Save to ollama_responses.txt
```

**PowerShell (`ollama_simple.ps1`)**
```powershell
# Equivalent flow:
1. Test-OllamaConnection â†’ Same verification
2. Get-ProjectFiles â†’ Same file reading
3. Invoke-OllamaQuery â†’ Same API query
4. Save-Response â†’ Same saving
```

#### 3. **Context Mechanism**
- **Input**: User question + project files
- **Processing**: 
  - Scan patterns: `*.py`, `*.js`, `*.html`, `*.css`, `*.json`, `*.md`, `*.txt`
  - Read content (first 2000 characters per file)
  - Build structured prompt
- **Output**: Ollama response + buffer save

#### 4. **Model Management**
```python
# Selection algorithm:
for model in available_models:
    size_mb = model.size / (1024 * 1024)
    if size_mb > largest_size:
        best_model = model.name
```

#### 5. **Error Handling**
- **Connection**: 5s timeout for verification, 60s for queries
- **Models**: Fallback if no models available
- **Files**: Skip large or inaccessible files
- **Encoding**: UTF-8 with error handling

#### 6. **Data Flow**
```
User â†’ Script (Python/PowerShell) â†’ Ollama API â†’ Local Model â†’ Response â†’ File
```

#### 7. **Technical Features**
- **Buffer**: Maintains last 10 responses
- **Format**: Timestamp + Q&A in `ollama_responses.txt`
- **Cross-platform**: Windows batch scripts for easy execution
- **Modular**: Python/PowerShell separation for flexibility

#### 8. **Use Cases**
- Project-specific code analysis
- Local debugging with context
- Automatic documentation
- Technical queries with project knowledge

This system acts as an **intelligent bridge** between local development environment and local AI model, providing project-specific context for more accurate queries.

## ğŸ“„ License

This project is licensed under the MIT License.

## ğŸ™ Acknowledgments

- **Ollama**: For making local AI accessible
- **Cursor AI**: For being an excellent editor
- **GitHub**: For hosting this repository

---

*Developed by [aoxilus](https://github.com/aoxilus)*
